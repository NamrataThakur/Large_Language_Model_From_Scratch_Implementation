[2025-08-07 09:07:30,612.612] Namespace(experiment_name='IFT_Exp_MaskedInst_v2_i3_LoRA_noGC', base_modelName='gpt2_355M', data_path='instruction-data-NT.json', training_type='IFT', peft_type='lora', load_weights=True, pre_save_model='gpt2_355M_nonMaskedInstruct_FineTuned_v2.pth', model_name='gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC', tokenizer='tiktoken', seed=123, batch_size=8, train_split=0.85, val_split=0.05, context_length=1024, max_new_tokens=256, temp=0.0, dropout_rate=0.1, top_k=None, trainable_layers=None, num_epochs=2, eos_id=50256, max_training_length='longest_training_example', prompt_style='alpaca', ignore_index=-100, mask_instruction=True, use_warmup=False, use_gradient_clip=False, warmup_steps=20, initial_lr=1e-05, min_lr=1e-05, beta=0.1, lora_rank=8, lora_alpha=8)
[2025-08-07 09:07:33,282.282] Configuration of the gpt2_355M base model loaded..!
[2025-08-07 09:07:33,282.282] Extention detected for the training file is "json".
[2025-08-07 09:07:33,283.283] Reading for .json files..!
[2025-08-07 09:07:33,285.285] Number of entries : 1100. 
[2025-08-07 09:07:33,285.285] Example of data for Instruct Fine-Tune :: 
 {'instruction': 'Name three forms of water.', 'input': '', 'output': 'The three forms of water are solid (ice), liquid (water), and gas (steam).'}
[2025-08-07 09:07:33,287.287] Training, Validation and Test Data created from the training file. Train data: 935, Val Data: 55, Test Data: 110
[2025-08-07 09:07:33,288.288] Loading the dataset class for instruction fine-tuning task...
[2025-08-07 09:07:34,410.410] ************** TRAIN DATALOADER ****************************
[2025-08-07 09:07:34,410.410] Length of Train Dataloader (number of batches): 116
[2025-08-07 09:07:34,964.964] torch.Size([8, 62]), torch.Size([8, 62])
[2025-08-07 09:07:34,964.964] torch.Size([8, 77]), torch.Size([8, 77])
[2025-08-07 09:07:34,964.964] torch.Size([8, 74]), torch.Size([8, 74])
[2025-08-07 09:07:34,964.964] torch.Size([8, 69]), torch.Size([8, 69])
[2025-08-07 09:07:34,970.970] torch.Size([8, 66]), torch.Size([8, 66])
[2025-08-07 09:07:34,970.970] ************** VAL DATALOADER ****************************
[2025-08-07 09:07:34,970.970] Length of Val Dataloader (number of batches): 6
[2025-08-07 09:07:34,970.970] torch.Size([8, 63]), torch.Size([8, 63])
[2025-08-07 09:07:34,970.970] torch.Size([8, 84]), torch.Size([8, 84])
[2025-08-07 09:07:34,970.970] torch.Size([8, 59]), torch.Size([8, 59])
[2025-08-07 09:07:34,970.970] torch.Size([8, 70]), torch.Size([8, 70])
[2025-08-07 09:07:34,970.970] torch.Size([8, 67]), torch.Size([8, 67])
[2025-08-07 09:07:34,970.970] ************** TEST DATALOADER ****************************
[2025-08-07 09:07:34,970.970] Length of Test Dataloader (number of batches): 13
[2025-08-07 09:07:34,970.970] torch.Size([8, 77]), torch.Size([8, 77])
[2025-08-07 09:07:34,970.970] torch.Size([8, 69]), torch.Size([8, 69])
[2025-08-07 09:07:34,970.970] torch.Size([8, 69]), torch.Size([8, 69])
[2025-08-07 09:07:34,970.970] torch.Size([8, 65]), torch.Size([8, 65])
[2025-08-07 09:07:34,970.970] torch.Size([8, 73]), torch.Size([8, 73])
[2025-08-07 09:07:34,970.970] Dataloaders created successfully for fine-tuning task..!
[2025-08-07 09:07:34,970.970] ---------------------------------------------------------
[2025-08-07 09:07:34,970.970] Loading the weights of the model : gpt2_355M_nonMaskedInstruct_FineTuned_v2.pth..!
[2025-08-07 09:07:34,970.970] Model present in the path: model/gpt2_355M_nonMaskedInstruct_FineTuned_v2.pth
[2025-08-07 09:07:37,858.858] Model weights loaded successfully..!
[2025-08-07 09:07:37,858.858] Generate a text to check if loading is successful..!
[2025-08-07 09:07:40,067.067] Generating a text :: 
Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: What is the capital of Denmark?### Response:The capital of Denmark is Copenhagen.
[2025-08-07 09:07:40,067.067] Instruction Fine-tuning the base model: gpt2_355M ..!
[2025-08-07 09:07:40,067.067] Paramater efficient mechanisms given is lora..!
[2025-08-07 09:07:40,069.069] Total trainable paramters in the original model: 406286336.
[2025-08-07 09:07:40,105.105] Total parameters in the model after LORA addition: 410235528
[2025-08-07 09:07:40,105.105] Total trainable parameters with LORA (%): 0.97 .
[2025-08-07 09:07:40,107.107] Training Stage : LoRA Layers Added ..!
[2025-08-07 09:07:40,515.515] Training Stage : Model sent to cuda for fine-tuning..!
[2025-08-07 09:07:40,524.524] Training Stage : Fine-tuning of the model started ..!
[2025-08-07 09:07:43,310.310] Maximum Learning Rate : 5e-05.
[2025-08-07 09:07:43,310.310] Total training steps : 232.
[2025-08-07 09:07:47,499.499] Epoch No: 1, Step: 000000, Train Loss: 0.581, Val Loss: 0.827

[2025-08-07 09:07:47,499.499] Total Tokens seen till now: 496

[2025-08-07 09:07:52,610.610] BEST model SAVED on iteration 000000 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:08:05,583.583] Epoch No: 1, Step: 000005, Train Loss: 0.637, Val Loss: 0.815

[2025-08-07 09:08:05,583.583] Total Tokens seen till now: 3368

[2025-08-07 09:08:22,469.469] BEST model SAVED on iteration 000005 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:08:35,666.666] Epoch No: 1, Step: 000010, Train Loss: 0.634, Val Loss: 0.751

[2025-08-07 09:08:35,666.666] Total Tokens seen till now: 6168

[2025-08-07 09:08:41,494.494] BEST model SAVED on iteration 000010 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:08:54,774.774] Epoch No: 1, Step: 000015, Train Loss: 0.621, Val Loss: 0.824

[2025-08-07 09:08:54,774.774] Total Tokens seen till now: 9080

[2025-08-07 09:09:11,106.106] Epoch No: 1, Step: 000020, Train Loss: 0.496, Val Loss: 0.804

[2025-08-07 09:09:11,106.106] Total Tokens seen till now: 12056

[2025-08-07 09:09:21,834.834] Epoch No: 1, Step: 000025, Train Loss: 0.579, Val Loss: 0.846

[2025-08-07 09:09:21,834.834] Total Tokens seen till now: 14736

[2025-08-07 09:09:33,203.203] Epoch No: 1, Step: 000030, Train Loss: 0.632, Val Loss: 0.780

[2025-08-07 09:09:33,203.203] Total Tokens seen till now: 17432

[2025-08-07 09:09:47,279.279] Epoch No: 1, Step: 000035, Train Loss: 0.649, Val Loss: 0.746

[2025-08-07 09:09:47,279.279] Total Tokens seen till now: 20304

[2025-08-07 09:09:50,404.404] BEST model SAVED on iteration 000035 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:10:02,843.843] Epoch No: 1, Step: 000040, Train Loss: 0.555, Val Loss: 0.869

[2025-08-07 09:10:02,843.843] Total Tokens seen till now: 23032

[2025-08-07 09:10:23,684.684] Epoch No: 1, Step: 000045, Train Loss: 0.576, Val Loss: 0.718

[2025-08-07 09:10:23,684.684] Total Tokens seen till now: 26272

[2025-08-07 09:10:30,172.172] BEST model SAVED on iteration 000045 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:10:43,612.612] Epoch No: 1, Step: 000050, Train Loss: 0.540, Val Loss: 0.843

[2025-08-07 09:10:43,612.612] Total Tokens seen till now: 29104

[2025-08-07 09:10:58,290.290] Epoch No: 1, Step: 000055, Train Loss: 0.499, Val Loss: 0.741

[2025-08-07 09:10:58,290.290] Total Tokens seen till now: 31944

[2025-08-07 09:11:10,537.537] Epoch No: 1, Step: 000060, Train Loss: 0.576, Val Loss: 0.742

[2025-08-07 09:11:10,537.537] Total Tokens seen till now: 34688

[2025-08-07 09:11:21,191.191] Epoch No: 1, Step: 000065, Train Loss: 0.598, Val Loss: 0.840

[2025-08-07 09:11:21,191.191] Total Tokens seen till now: 37336

[2025-08-07 09:11:38,559.559] Epoch No: 1, Step: 000070, Train Loss: 0.469, Val Loss: 0.750

[2025-08-07 09:11:38,560.560] Total Tokens seen till now: 40232

[2025-08-07 09:11:49,754.754] Epoch No: 1, Step: 000075, Train Loss: 0.486, Val Loss: 0.812

[2025-08-07 09:11:49,754.754] Total Tokens seen till now: 42928

[2025-08-07 09:12:01,034.034] Epoch No: 1, Step: 000080, Train Loss: 0.643, Val Loss: 0.842

[2025-08-07 09:12:01,034.034] Total Tokens seen till now: 45568

[2025-08-07 09:12:12,643.643] Epoch No: 1, Step: 000085, Train Loss: 0.540, Val Loss: 0.628

[2025-08-07 09:12:12,643.643] Total Tokens seen till now: 48256

[2025-08-07 09:12:17,700.700] BEST model SAVED on iteration 000085 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:12:31,182.182] Epoch No: 1, Step: 000090, Train Loss: 0.446, Val Loss: 0.652

[2025-08-07 09:12:31,182.182] Total Tokens seen till now: 51112

[2025-08-07 09:12:45,823.823] Epoch No: 1, Step: 000095, Train Loss: 0.420, Val Loss: 0.805

[2025-08-07 09:12:45,823.823] Total Tokens seen till now: 54168

[2025-08-07 09:13:00,839.839] Epoch No: 1, Step: 000100, Train Loss: 0.518, Val Loss: 0.554

[2025-08-07 09:13:00,839.839] Total Tokens seen till now: 57152

[2025-08-07 09:13:05,356.356] BEST model SAVED on iteration 000100 to model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..! 
[2025-08-07 09:13:19,966.966] Epoch No: 1, Step: 000105, Train Loss: 0.369, Val Loss: 0.671

[2025-08-07 09:13:19,966.966] Total Tokens seen till now: 60072

[2025-08-07 09:13:33,162.162] Epoch No: 1, Step: 000110, Train Loss: 0.434, Val Loss: 0.763

[2025-08-07 09:13:33,162.162] Total Tokens seen till now: 62904

[2025-08-07 09:13:47,434.434] Epoch No: 1, Step: 000115, Train Loss: 0.308, Val Loss: 0.720

[2025-08-07 09:13:47,434.434] Total Tokens seen till now: 65760

[2025-08-07 09:13:49,008.008] Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: Rewrite the sentence using a simile.### Input: The car is very fast.### Response:The car is as fast as a horse.
[2025-08-07 09:14:00,074.074] Epoch No: 2, Step: 000120, Train Loss: 0.499, Val Loss: 0.677

[2025-08-07 09:14:00,074.074] Total Tokens seen till now: 68408

[2025-08-07 09:14:10,322.322] Epoch No: 2, Step: 000125, Train Loss: 0.362, Val Loss: 0.653

[2025-08-07 09:14:10,322.322] Total Tokens seen till now: 71024

[2025-08-07 09:14:21,768.768] Epoch No: 2, Step: 000130, Train Loss: 0.386, Val Loss: 0.794

[2025-08-07 09:14:21,768.768] Total Tokens seen till now: 73760

[2025-08-07 09:14:33,322.322] Epoch No: 2, Step: 000135, Train Loss: 0.461, Val Loss: 0.743

[2025-08-07 09:14:33,322.322] Total Tokens seen till now: 76440

[2025-08-07 09:14:45,663.663] Epoch No: 2, Step: 000140, Train Loss: 0.389, Val Loss: 0.730

[2025-08-07 09:14:45,679.679] Total Tokens seen till now: 79208

[2025-08-07 09:14:57,357.357] Epoch No: 2, Step: 000145, Train Loss: 0.393, Val Loss: 0.719

[2025-08-07 09:14:57,357.357] Total Tokens seen till now: 81944

[2025-08-07 09:15:08,855.855] Epoch No: 2, Step: 000150, Train Loss: 0.303, Val Loss: 0.686

[2025-08-07 09:15:08,855.855] Total Tokens seen till now: 84728

[2025-08-07 09:15:28,086.086] Epoch No: 2, Step: 000155, Train Loss: 0.485, Val Loss: 0.727

[2025-08-07 09:15:28,086.086] Total Tokens seen till now: 87840

[2025-08-07 09:15:45,742.742] Epoch No: 2, Step: 000160, Train Loss: 0.447, Val Loss: 0.820

[2025-08-07 09:15:45,742.742] Total Tokens seen till now: 90896

[2025-08-07 09:16:01,112.112] Epoch No: 2, Step: 000165, Train Loss: 0.336, Val Loss: 0.761

[2025-08-07 09:16:01,112.112] Total Tokens seen till now: 93912

[2025-08-07 09:16:21,321.321] Epoch No: 2, Step: 000170, Train Loss: 0.323, Val Loss: 0.816

[2025-08-07 09:16:21,321.321] Total Tokens seen till now: 97024

[2025-08-07 09:16:36,533.533] Epoch No: 2, Step: 000175, Train Loss: 0.305, Val Loss: 0.773

[2025-08-07 09:16:36,533.533] Total Tokens seen till now: 99944

[2025-08-07 09:16:49,681.681] Epoch No: 2, Step: 000180, Train Loss: 0.320, Val Loss: 0.816

[2025-08-07 09:16:49,681.681] Total Tokens seen till now: 102704

[2025-08-07 09:17:02,700.700] Epoch No: 2, Step: 000185, Train Loss: 0.398, Val Loss: 0.788

[2025-08-07 09:17:02,700.700] Total Tokens seen till now: 105512

[2025-08-07 09:17:14,451.451] Epoch No: 2, Step: 000190, Train Loss: 0.367, Val Loss: 0.738

[2025-08-07 09:17:14,451.451] Total Tokens seen till now: 108192

[2025-08-07 09:17:25,441.441] Epoch No: 2, Step: 000195, Train Loss: 0.349, Val Loss: 0.727

[2025-08-07 09:17:25,441.441] Total Tokens seen till now: 110816

[2025-08-07 09:17:39,910.910] Epoch No: 2, Step: 000200, Train Loss: 0.267, Val Loss: 0.726

[2025-08-07 09:17:39,910.910] Total Tokens seen till now: 113696

[2025-08-07 09:17:49,958.958] Epoch No: 2, Step: 000205, Train Loss: 0.387, Val Loss: 0.615

[2025-08-07 09:17:49,958.958] Total Tokens seen till now: 116216

[2025-08-07 09:18:01,733.733] Epoch No: 2, Step: 000210, Train Loss: 0.338, Val Loss: 0.683

[2025-08-07 09:18:01,733.733] Total Tokens seen till now: 119048

[2025-08-07 09:18:19,171.171] Epoch No: 2, Step: 000215, Train Loss: 0.226, Val Loss: 0.773

[2025-08-07 09:18:19,171.171] Total Tokens seen till now: 122104

[2025-08-07 09:18:32,914.914] Epoch No: 2, Step: 000220, Train Loss: 0.279, Val Loss: 0.722

[2025-08-07 09:18:32,916.916] Total Tokens seen till now: 125016

[2025-08-07 09:18:49,167.167] Epoch No: 2, Step: 000225, Train Loss: 0.327, Val Loss: 0.782

[2025-08-07 09:18:49,167.167] Total Tokens seen till now: 127976

[2025-08-07 09:19:02,484.484] Epoch No: 2, Step: 000230, Train Loss: 0.307, Val Loss: 0.749

[2025-08-07 09:19:02,484.484] Total Tokens seen till now: 130760

[2025-08-07 09:19:06,361.361] Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: Rewrite the sentence using a simile.### Input: The car is very fast.### Response:The car is as fast as a cheetah.
[2025-08-07 09:19:06,362.362] Training completed in 11.43 minutes.
[2025-08-07 09:19:06,363.363] BEST Instruction Fine-Tuned (IFT) model saved in model/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC.pth..!
[2025-08-07 09:19:06,363.363] Saving the plots of the metrics tracked ..!
[2025-08-07 09:19:09,733.733] Saving the model response for the test dataset ..!
[2025-08-07 09:22:13,552.552] Model response for the test dataset saved in data/gpt2_355M_MaskedInstruct_FineTuned_v2_i3_LoRA_noGC_testdata_response.json..!
[2025-08-07 09:22:13,552.552] Pipeline completed in 14.55 minutes.
