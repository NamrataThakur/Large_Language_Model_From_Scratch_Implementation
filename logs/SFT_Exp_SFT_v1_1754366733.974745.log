[2025-08-05 09:35:33,974.974] Namespace(experiment_name='SFT_Exp_SFT_v1', base_modelName='gpt2_124M', data_path='sms_spam_collection.zip', training_type='SFT', peft_type=None, load_weights=True, pre_save_model=None, model_name='gpt2_124M_SFT_Spam_v1', tokenizer='tiktoken', seed=123, batch_size=8, train_split=0.7, val_split=0.1, context_length=1024, max_new_tokens=256, temp=0.0, dropout_rate=0.0, top_k=5, trainable_layers='last_block', num_epochs=5, eos_id=50256, max_training_length='longest_training_example', prompt_style='alpaca', ignore_index=-100, mask_instruction=False, use_warmup=False, use_gradient_clip=False, warmup_steps=20, initial_lr=1e-05, min_lr=1e-05, beta=0.1, lora_rank=16, lora_alpha=16)
[2025-08-05 09:35:35,165.165] Configuration of the gpt2_124M base model loaded..!
[2025-08-05 09:35:35,165.165] Extention detected for the training file is "zip".
[2025-08-05 09:35:35,165.165] Unzipping the file
[2025-08-05 09:35:35,184.184] File unzipped and saved at: data\sms_spam_collection\SMSSpamCollection.tsv
[2025-08-05 09:35:35,212.212] Total records present in the training file: (5572, 2)
[2025-08-05 09:35:35,217.217] After balancing : (1494, 2)
[2025-08-05 09:35:35,227.227] Training, Validation and Test Data created from the training file. Train data: (1045, 2), Val Data: (149, 2), Test Data: (300, 2)
[2025-08-05 09:35:35,227.227] ---------------------------------------------------------
[2025-08-05 09:35:35,227.227] Loading the dataset class for supervised classification fine-tuning task...
[2025-08-05 09:35:35,264.264] ************** TRAIN DATALOADER ****************************
[2025-08-05 09:35:35,264.264] Length of Train Dataloader (number of batches): 130
[2025-08-05 09:35:35,267.267] torch.Size([8, 120]), torch.Size([8])
[2025-08-05 09:35:35,267.267] Longest Training Example Length : 120.
[2025-08-05 09:35:35,279.279] ************** VAL DATALOADER ****************************
[2025-08-05 09:35:35,279.279] Length of Val Dataloader (number of batches): 18
[2025-08-05 09:35:35,280.280] torch.Size([8, 120]), torch.Size([8])
[2025-08-05 09:35:35,280.280] ************** TEST DATALOADER ****************************
[2025-08-05 09:35:35,280.280] Length of Test Dataloader (number of batches): 37
[2025-08-05 09:35:35,281.281] torch.Size([8, 120]), torch.Size([8])
[2025-08-05 09:35:35,281.281] Dataloaders created successfully for classification fine-tuning task..!
[2025-08-05 09:35:35,281.281] ---------------------------------------------------------
[2025-08-05 09:35:35,281.281] Loading the weights of the base model : gpt2_124M..!
[2025-08-05 09:35:35,281.281] Model present in the path: model/gpt2
[2025-08-05 09:35:44,060.060] Model weights loaded successfully..!
[2025-08-05 09:35:44,060.060] Generate a text to check if loading is successful..!
[2025-08-05 09:36:13,493.493] Generating a text :: 
Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of
[2025-08-05 09:36:13,493.493] Training the full model as no paramater efficient mechanisms are given..!
[2025-08-05 09:36:13,493.493] Training Stage : Frozen the original paramters of the model..!
[2025-08-05 09:36:13,493.493] Training Stage : Added the NEW classification head..!
[2025-08-05 09:36:13,493.493] ************* Verifying the NEW output head of the model *************
[2025-08-05 09:36:13,522.522] Output Dimension:: torch.Size([1, 4, 2]) .
[2025-08-05 09:36:13,522.522] ************* Verifying the NEW output head of the model : Successfull *************
[2025-08-05 09:36:13,524.524] Training Stage : Unfreezing the weights of last block of the model for fine-tuning..!
[2025-08-05 09:36:14,926.926] Training Stage : Model sent to cuda for fine-tuning..!
[2025-08-05 09:36:15,039.039] Training Stage : Fine-tuning of the model started ..!
[2025-08-05 09:36:24,689.689] Maximum Learning Rate : 0.0005.
[2025-08-05 09:36:24,689.689] Total training steps : 650.
[2025-08-05 09:36:26,741.741] Epoch No: 1, Step: 000000, Train Loss: 1.669, Test Loss: 1.241
[2025-08-05 09:36:27,723.723] BEST model SAVED on iteration 000000 to model/gpt2_124M_SFT_Spam_v1.pth..! 
[2025-08-05 09:36:35,845.845] Epoch No: 1, Step: 000050, Train Loss: 0.060, Test Loss: 0.017
[2025-08-05 09:36:36,839.839] BEST model SAVED on iteration 000050 to model/gpt2_124M_SFT_Spam_v1.pth..! 
[2025-08-05 09:36:44,992.992] Epoch No: 1, Step: 000100, Train Loss: 0.018, Test Loss: 0.112
[2025-08-05 09:37:07,005.005] Training accuracy: 98.08%
[2025-08-05 09:37:07,005.005] Validation accuracy: 97.92%
[2025-08-05 09:37:11,188.188] Epoch No: 2, Step: 000150, Train Loss: 0.024, Test Loss: 0.063
[2025-08-05 09:37:19,424.424] Epoch No: 2, Step: 000200, Train Loss: 0.023, Test Loss: 0.122
[2025-08-05 09:37:27,678.678] Epoch No: 2, Step: 000250, Train Loss: 0.011, Test Loss: 0.076
[2025-08-05 09:37:47,035.035] Training accuracy: 99.23%
[2025-08-05 09:37:47,035.035] Validation accuracy: 98.61%
[2025-08-05 09:37:54,065.065] Epoch No: 3, Step: 000300, Train Loss: 0.009, Test Loss: 0.045
[2025-08-05 09:38:02,350.350] Epoch No: 3, Step: 000350, Train Loss: 0.019, Test Loss: 0.021
[2025-08-05 09:38:26,005.005] Training accuracy: 99.52%
[2025-08-05 09:38:26,005.005] Validation accuracy: 97.92%
[2025-08-05 09:38:28,794.794] Epoch No: 4, Step: 000400, Train Loss: 0.123, Test Loss: 0.001
[2025-08-05 09:38:29,815.815] BEST model SAVED on iteration 000400 to model/gpt2_124M_SFT_Spam_v1.pth..! 
[2025-08-05 09:38:38,095.095] Epoch No: 4, Step: 000450, Train Loss: 0.005, Test Loss: 0.265
[2025-08-05 09:38:46,405.405] Epoch No: 4, Step: 000500, Train Loss: 0.001, Test Loss: 0.241
[2025-08-05 09:39:07,270.270] Training accuracy: 94.13%
[2025-08-05 09:39:07,270.270] Validation accuracy: 97.92%
[2025-08-05 09:39:12,898.898] Epoch No: 5, Step: 000550, Train Loss: 0.170, Test Loss: 0.083
[2025-08-05 09:39:21,197.197] Epoch No: 5, Step: 000600, Train Loss: 0.057, Test Loss: 0.005
[2025-08-05 09:39:46,353.353] Training accuracy: 99.33%
[2025-08-05 09:39:46,353.353] Validation accuracy: 97.92%
[2025-08-05 09:39:46,368.368] Training completed in 3.52 minutes.
[2025-08-05 09:39:46,368.368] SFT Fine-Tuned model saved in model/gpt2_124M_SFT_Spam_v1.pth..!
[2025-08-05 09:39:46,368.368] Saving the plots of the metrics tracked ..!
[2025-08-05 09:40:15,816.816] Training accuracy: 97.50%
[2025-08-05 09:40:15,816.816] Validation accuracy: 97.50%
[2025-08-05 09:40:15,816.816] Test accuracy: 96.25%
[2025-08-05 09:40:15,816.816] Saving the model response for the test dataset ..!
[2025-08-05 09:40:21,656.656] Model response for the test dataset saved in data/gpt2_124M_SFT_Spam_v1_testdata_response.csv..!
