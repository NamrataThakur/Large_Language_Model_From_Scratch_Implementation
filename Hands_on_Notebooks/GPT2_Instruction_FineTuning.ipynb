{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89762420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.24.3\n",
      "matplotlib version: 3.9.2\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n",
      "tqdm version: 4.66.6\n",
      "tensorflow version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import tiktoken\n",
    "import torch\n",
    "import urllib.request\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from importlib.metadata import version\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "import ollama\n",
    "import psutil\n",
    "import urllib.request\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08862e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'D:\\LLM_Deeplearning.ai\\SEBASTIAN_RASCHKA\\LLMs-from-scratch-main\\LLMs-from-scratch-main\\ch04\\Transformer_Implementation')\n",
    "sys.path.insert(2, 'D:\\LLM_Deeplearning.ai\\SEBASTIAN_RASCHKA\\LLMs-from-scratch-main\\LLMs-from-scratch-main\\ch05\\CustomGPT_Pretraining')\n",
    "\n",
    "from text_generation import Text_Generation\n",
    "from gpt2 import GPT2\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from metrics import Metrics\n",
    "from plot_metrics import Plots\n",
    "from gpt2_pretrain import GPT2_PreTrain\n",
    "from load_model_weights import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de20307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example:: \n",
      " {'instruction': 'Name three forms of water.', 'input': '', 'output': 'The three forms of water are solid (ice), liquid (water), and gas (steam).'}\n"
     ]
    }
   ],
   "source": [
    "#The lines of code for downloading file is taken as in from the ch07.ipynb file of code repo of \"Build LLM From Scratch\".\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data-NT.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "print('Example:: \\n',data[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1c9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_response(input_json, prompt_style = 'alpaca', inference=False):\n",
    "\n",
    "    if prompt_style == 'alpaca':\n",
    "        instruction = (f\"Below is an instruction that describes a task. \"\n",
    "                       f\"Write a response that appropriately completes the request.\"\n",
    "                       f\"\\n\\n### Instruction:\\n {input_json['instruction']}\"\n",
    "                    )\n",
    "        input = f\"\\n\\n### Input:\\n {input_json['input']}\" if input_json['input'] else \"\"\n",
    "\n",
    "        response = f\"\\n\\n### Response:\\n{input_json['output']}\\n\"\n",
    "\n",
    "        formatted_input_with_response = instruction + input + response\n",
    "        instruction_length = len(instruction + input)\n",
    "        inf_format = instruction + input \n",
    "\n",
    "    else:\n",
    "\n",
    "        instruction = f\"\\n\\n<|user|>\\n {input_json['instruction']} \"\n",
    "                            \n",
    "        input = f\": {input_json['input']}\" if input_json['input'] else \"\"\n",
    "\n",
    "        response =f\"\\n\\n<|assistant|>\\n {input_json['output']}\"\n",
    "                    \n",
    "\n",
    "        formatted_input_with_response = instruction + input + response\n",
    "        instruction_length = len(instruction + input)\n",
    "        inf_format = instruction + input \n",
    "\n",
    "    return instruction_length, inf_format if inference else formatted_input_with_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10a4bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca Style formatted input prompt ::\n",
      "\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " Rewrite the sentence using an idiom.\n",
      "\n",
      "### Input:\n",
      " He is very rich.\n",
      "\n",
      "### Response:\n",
      "He is rolling in dough.\n",
      "\n",
      "Instruction Length ::  191\n",
      "---------------------------------------------------------\n",
      "Phi3 Style formatted input prompt :: \n",
      "\n",
      "<|user|>\n",
      " Rewrite the sentence using an idiom. : He is very rich.\n",
      "\n",
      "<|assistant|>\n",
      " He is rolling in dough.\n",
      "Instruction Length ::  67\n"
     ]
    }
   ],
   "source": [
    "instruction_length, formatted_input_with_response = format_input_response(data[900], 'alpaca')\n",
    "print('Alpaca Style formatted input prompt ::\\n\\n', formatted_input_with_response)\n",
    "print('Instruction Length :: ', instruction_length)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "instruction_length, formatted_input_with_response = format_input_response(data[900], 'phi3')\n",
    "print('Phi3 Style formatted input prompt ::', formatted_input_with_response)\n",
    "print('Instruction Length :: ', instruction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f149fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Val Split Index ::  935 990\n",
      "Train Data:  935\n",
      "Val Data:  55\n",
      "Test Data:  110\n",
      "\n",
      "Example Train Data :\n",
      "\n",
      " {'instruction': 'Rewrite the sentence using an idiom.', 'input': 'He is very rich.', 'output': 'He is rolling in dough.'}\n"
     ]
    }
   ],
   "source": [
    "def dataset_split(data, train_split, val_split):\n",
    "\n",
    "    #Create the split indices:\n",
    "    train_df = data[ : int(train_split * len(data))]\n",
    "    val_df = data[int(train_split * len(data)) : int(train_split * len(data)) + int(val_split * len(data))]\n",
    "    test_df = data[int(train_split * len(data)) + int(val_split * len(data)) : ]\n",
    "\n",
    "    print('Train and Val Split Index :: ',int(train_split * len(data)), int(train_split * len(data)) + int(val_split * len(data)))\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = dataset_split(data, 0.85, 0.05)\n",
    "\n",
    "print('Train Data: ', len(train_df))\n",
    "print('Val Data: ', len(val_df))\n",
    "print('Test Data: ', len(test_df))\n",
    "\n",
    "print('\\nExample Train Data :\\n\\n', train_df[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024ba960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTCustomInstructionDataset(Dataset):\n",
    "    def __init__(self, input_data, tokenizer, prompt_style):\n",
    "        self.encoded_data = []\n",
    "        self.instruction_length = []\n",
    "        self.data = input_data\n",
    "\n",
    "        for row in input_data:\n",
    "\n",
    "            instruct_length, formatted_row = format_input_response(row, prompt_style)\n",
    "            tokenized_row = tokenizer.encode(formatted_row, allowed_special = 'all')\n",
    "            self.encoded_data.append(tokenized_row)\n",
    "            self.instruction_length.append(instruct_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.instruction_length[index], self.encoded_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206d76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_instruct(batch, pad_token = 50256, ignore_index = -100, device = 'cpu', \n",
    "                            max_seq_length = None, mask_instruction = False):\n",
    "\n",
    "    batch_max_length = max([len(item)+1 for instruction_length, item in batch])\n",
    "    input_list, target_list = [], []\n",
    "\n",
    "    for instruction_length, item in batch:\n",
    "        tokens = item.copy()\n",
    "        tokens = tokens + [pad_token]\n",
    "\n",
    "        padded_tokens = tokens + [pad_token] * (batch_max_length - len(tokens))\n",
    "\n",
    "        inputs = torch.tensor(padded_tokens[:-1])\n",
    "        targets = torch.tensor(padded_tokens[1:])\n",
    "\n",
    "        mask = targets == pad_token\n",
    "        idx = torch.nonzero(mask).squeeze()\n",
    "        if idx.numel() > 1:\n",
    "            targets[idx[1:]] = ignore_index\n",
    "\n",
    "        if mask_instruction:\n",
    "            targets[:instruction_length] = ignore_index\n",
    "\n",
    "        if max_seq_length is not None:\n",
    "            inputs = inputs[:max_seq_length]\n",
    "            targets = targets[:max_seq_length]\n",
    "\n",
    "        input_list.append(inputs)\n",
    "        target_list.append(targets)\n",
    "\n",
    "\n",
    "    input_tensor = torch.stack(input_list).to(device)\n",
    "    target_tensor = torch.stack(target_list).to(device)\n",
    "\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d930632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Collate Function Performance :\n",
      "Without Instruction Masking :: \n",
      "Input:  tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Target:  tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n",
      "------------------\n",
      "With Instruction Masking :: \n",
      "Input:  tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Target:  tensor([[ -100,  -100,     3,     4, 50256],\n",
      "        [ -100, 50256,  -100,  -100,  -100],\n",
      "        [ -100,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = (2,[0, 1, 2, 3, 4])\n",
    "inputs_2 = (1,[5, 6])\n",
    "inputs_3 = (1,[7, 8, 9])\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print('Checking Collate Function Performance :')\n",
    "\n",
    "print('Without Instruction Masking :: ')\n",
    "i, t = custom_collate_instruct(batch=batch)\n",
    "print('Input: ', i)\n",
    "print('Target: ', t)\n",
    "print('------------------')\n",
    "\n",
    "print('With Instruction Masking :: ')\n",
    "i , t =custom_collate_instruct(batch=batch, mask_instruction=True)\n",
    "print('Input: ', i)\n",
    "print('Target: ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560da42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 1024)\n",
       "  (pos_embedding): Embedding(1024, 1024)\n",
       "  (token_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_block): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_layerNorm): LayerNormalization()\n",
       "  (final_projection): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model config:\n",
    "def load_model_config(base_config,model_name,config_dict):\n",
    "\n",
    "    updated_config = base_config.copy()\n",
    "    updated_config.update(config_dict[model_name])\n",
    "    updated_config.update({'qkv_bias':True, 'context_length':1024})\n",
    "\n",
    "    return updated_config\n",
    "\n",
    "# Base configs:\n",
    "GPT2_CONFIG = {\n",
    "    'vocab_size':50257,\n",
    "    'embedding_dimension':768,\n",
    "    'num_heads':12,\n",
    "    'context_length':256, #We used a smaller context length till now to do quick training of the model to make sure the code is working properly.\n",
    "    'dropout':0.1,\n",
    "    'qkv_bias':False,\n",
    "    'num_layers':12,\n",
    "    }\n",
    "\n",
    "gpt2_config_dict = {\n",
    "    \"gpt2_124M\" : {'embedding_dimension':768, 'num_heads':12, 'num_layers':12},\n",
    "    \"gpt2_355M\" : {'embedding_dimension':1024, 'num_heads':16, 'num_layers':24},\n",
    "    \"gpt2_774M\" : {'embedding_dimension':1280, 'num_heads':20, 'num_layers':36},\n",
    "    \"gpt2_1558M\" : {'embedding_dimension':1600, 'num_heads':25, 'num_layers':48},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2_355M\"\n",
    "\n",
    "updatedGPT2_CONFIG = load_model_config(GPT2_CONFIG,model_name,gpt2_config_dict)\n",
    "\n",
    "gpt2_instruct = GPT2(updatedGPT2_CONFIG)\n",
    "\n",
    "#Get the OpenAI's parameters and settings loaded:\n",
    "settings, params = download_and_load_gpt2(model_size=\"355M\", models_dir=\"gpt2\")\n",
    "\n",
    "#Load the weights from OpenAI GPT2 to our instance:\n",
    "gpt2_loadedWeights(gpt2_instruct, params)\n",
    "gpt2_instruct.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94433aa0",
   "metadata": {},
   "source": [
    "##### In order to load the previously trained instruction fine-tuned model, run till the above code and then go to the last cell. Run that cell and the instruction fine-tuned weights will be loaded into the \"gpt2_instruct\" instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933341a6",
   "metadata": {},
   "source": [
    "##### In order to do the instruction fine-tuning for the first time, run all the below cells (In order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a4ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Available:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device Available: ', device)\n",
    "\n",
    "custom_collate_instruct = partial(custom_collate_instruct, device=device, max_seq_length=updatedGPT2_CONFIG['context_length'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ef5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the custom dataloader function that will call the GPTCustomInstructionDataset class to create the dataset from the given text:\n",
    "def GPTCustomInstructDataloader(data_file, pad_token = None, max_seq_length = None, batch_size=8, prompt_style = 'alpaca', \n",
    "                        shuffle=True, drop_last=True,num_workers=0):\n",
    "    \n",
    "    #Initializer the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    #Get the last token id of the tokenizer selected:\n",
    "    if pad_token is None:\n",
    "        pad_token = tokenizer.encode('<|endoftext|>', allowed_special='all')[0]\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    #Create the dataset with the tokenizer and the input file:\n",
    "    dataset = GPTCustomInstructionDataset(input_data = data_file, tokenizer=tokenizer, prompt_style=prompt_style)\n",
    "\n",
    "    #Create the dataloader with the dataset\n",
    "    custom_dataloader = DataLoader(dataset,batch_size=batch_size, collate_fn=custom_collate_instruct, \n",
    "                                   shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return custom_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efeb4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loader(loader):\n",
    "    i=0\n",
    "    for inputs, targets in loader:\n",
    "        print(inputs.shape, targets.shape)\n",
    "        if i > len(loader) - 2:\n",
    "            break\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ad0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches ::  116\n",
      "Train Loader ::\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "--------------------------\n",
      "Number of validation batches ::  6\n",
      "Validation Loader ::\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "--------------------------\n",
      "Number of test batches ::  13\n",
      "Test Loader ::\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loader = GPTCustomInstructDataloader(train_df)\n",
    "print('Number of training batches :: ', len(train_loader))\n",
    "\n",
    "print('Train Loader ::')\n",
    "print_loader(train_loader)\n",
    "print('--------------------------')\n",
    "\n",
    "val_loader = GPTCustomInstructDataloader(val_df)\n",
    "print('Number of validation batches :: ', len(val_loader))\n",
    "\n",
    "print('Validation Loader ::')\n",
    "print_loader(val_loader)\n",
    "print('--------------------------')\n",
    "\n",
    "test_loader = GPTCustomInstructDataloader(test_df)\n",
    "print('Number of test batches :: ', len(test_loader))\n",
    "\n",
    "print('Test Loader ::')\n",
    "print_loader(test_loader)\n",
    "print('--------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d9dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " What is the capital of Denmark?\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: What is the capital of Denmark?### Response:### Instruction: What is the capital of Denmark?### Response:### Instruction: What is the capital of Denmark?\n",
      "Generated Response ::  Response:### Instruction: What is the capital of Denmark?### Instruction: What is the capital of Denmark?\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "_, input_text = format_input_response(val_df[10], inference=True)\n",
    "print(input_text)\n",
    "print('-------------------------------------')\n",
    "generate = Text_Generation(model=gpt2_instruct, device='cpu', tokenizer_model='gpt2')\n",
    "output_text = generate.text_generation(input_text = input_text, max_new_tokens=35, temp=0.0,top_k= None, eos_id=50256)\n",
    "print(output_text)\n",
    "response = (output_text[len(input_text):]).replace(\"### Response:\", \"\").strip()\n",
    "print('Generated Response :: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2c4f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Train Loss :  3.905745911187139\n",
      "Initial Validation Loss :  4.035185178120931\n"
     ]
    }
   ],
   "source": [
    "#Calculate the initial loss on the instruction dataset:\n",
    "gpt2_instruct.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "metrics = Metrics(model=gpt2_instruct, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = metrics.loss_loader(train_loader)\n",
    "    val_loss = metrics.loss_loader(val_loader)\n",
    "\n",
    "print(\"Initial Train Loss : \", train_loss)\n",
    "print(\"Initial Validation Loss : \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9b0c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Available:  cuda\n",
      "Epoch No: 1, Step: 000000, Train Loss: 2.673, Test Loss: 2.740\n",
      "Total Tokens seen till now: 496\n",
      "Epoch No: 1, Step: 000005, Train Loss: 1.135, Test Loss: 1.140\n",
      "Total Tokens seen till now: 3368\n",
      "Epoch No: 1, Step: 000010, Train Loss: 0.953, Test Loss: 0.978\n",
      "Total Tokens seen till now: 6168\n",
      "Epoch No: 1, Step: 000015, Train Loss: 0.890, Test Loss: 0.949\n",
      "Total Tokens seen till now: 9080\n",
      "Epoch No: 1, Step: 000020, Train Loss: 0.809, Test Loss: 0.832\n",
      "Total Tokens seen till now: 12056\n",
      "Epoch No: 1, Step: 000025, Train Loss: 0.770, Test Loss: 0.851\n",
      "Total Tokens seen till now: 14736\n",
      "Epoch No: 1, Step: 000030, Train Loss: 0.736, Test Loss: 0.791\n",
      "Total Tokens seen till now: 17432\n",
      "Epoch No: 1, Step: 000035, Train Loss: 0.759, Test Loss: 0.745\n",
      "Total Tokens seen till now: 20304\n",
      "Epoch No: 1, Step: 000040, Train Loss: 0.683, Test Loss: 0.801\n",
      "Total Tokens seen till now: 23032\n",
      "Epoch No: 1, Step: 000045, Train Loss: 0.791, Test Loss: 0.720\n",
      "Total Tokens seen till now: 26272\n",
      "Epoch No: 1, Step: 000050, Train Loss: 0.734, Test Loss: 0.741\n",
      "Total Tokens seen till now: 29104\n",
      "Epoch No: 1, Step: 000055, Train Loss: 0.636, Test Loss: 0.721\n",
      "Total Tokens seen till now: 31944\n",
      "Epoch No: 1, Step: 000060, Train Loss: 0.692, Test Loss: 0.719\n",
      "Total Tokens seen till now: 34688\n",
      "Epoch No: 1, Step: 000065, Train Loss: 0.680, Test Loss: 0.753\n",
      "Total Tokens seen till now: 37336\n",
      "Epoch No: 1, Step: 000070, Train Loss: 0.578, Test Loss: 0.721\n",
      "Total Tokens seen till now: 40232\n",
      "Epoch No: 1, Step: 000075, Train Loss: 0.579, Test Loss: 0.746\n",
      "Total Tokens seen till now: 42928\n",
      "Epoch No: 1, Step: 000080, Train Loss: 0.700, Test Loss: 0.746\n",
      "Total Tokens seen till now: 45568\n",
      "Epoch No: 1, Step: 000085, Train Loss: 0.615, Test Loss: 0.646\n",
      "Total Tokens seen till now: 48256\n",
      "Epoch No: 1, Step: 000090, Train Loss: 0.568, Test Loss: 0.654\n",
      "Total Tokens seen till now: 51112\n",
      "Epoch No: 1, Step: 000095, Train Loss: 0.658, Test Loss: 0.736\n",
      "Total Tokens seen till now: 54168\n",
      "Epoch No: 1, Step: 000100, Train Loss: 0.582, Test Loss: 0.626\n",
      "Total Tokens seen till now: 57152\n",
      "Epoch No: 1, Step: 000105, Train Loss: 0.459, Test Loss: 0.654\n",
      "Total Tokens seen till now: 60072\n",
      "Epoch No: 1, Step: 000110, Train Loss: 0.553, Test Loss: 0.697\n",
      "Total Tokens seen till now: 62904\n",
      "Epoch No: 1, Step: 000115, Train Loss: 0.477, Test Loss: 0.651\n",
      "Total Tokens seen till now: 65760\n",
      "Epoch No: 2, Step: 000120, Train Loss: 0.542, Test Loss: 0.657\n",
      "Total Tokens seen till now: 68408\n",
      "Epoch No: 2, Step: 000125, Train Loss: 0.522, Test Loss: 0.657\n",
      "Total Tokens seen till now: 71024\n",
      "Epoch No: 2, Step: 000130, Train Loss: 0.505, Test Loss: 0.692\n",
      "Total Tokens seen till now: 73760\n",
      "Epoch No: 2, Step: 000135, Train Loss: 0.517, Test Loss: 0.700\n",
      "Total Tokens seen till now: 76440\n",
      "Epoch No: 2, Step: 000140, Train Loss: 0.511, Test Loss: 0.714\n",
      "Total Tokens seen till now: 79208\n",
      "Epoch No: 2, Step: 000145, Train Loss: 0.521, Test Loss: 0.673\n",
      "Total Tokens seen till now: 81944\n",
      "Epoch No: 2, Step: 000150, Train Loss: 0.418, Test Loss: 0.651\n",
      "Total Tokens seen till now: 84728\n",
      "Epoch No: 2, Step: 000155, Train Loss: 0.486, Test Loss: 0.654\n",
      "Total Tokens seen till now: 87840\n",
      "Epoch No: 2, Step: 000160, Train Loss: 0.503, Test Loss: 0.689\n",
      "Total Tokens seen till now: 90896\n",
      "Epoch No: 2, Step: 000165, Train Loss: 0.414, Test Loss: 0.682\n",
      "Total Tokens seen till now: 93912\n",
      "Epoch No: 2, Step: 000170, Train Loss: 0.475, Test Loss: 0.698\n",
      "Total Tokens seen till now: 97024\n",
      "Epoch No: 2, Step: 000175, Train Loss: 0.457, Test Loss: 0.705\n",
      "Total Tokens seen till now: 99944\n",
      "Epoch No: 2, Step: 000180, Train Loss: 0.433, Test Loss: 0.714\n",
      "Total Tokens seen till now: 102704\n",
      "Epoch No: 2, Step: 000185, Train Loss: 0.433, Test Loss: 0.699\n",
      "Total Tokens seen till now: 105512\n",
      "Epoch No: 2, Step: 000190, Train Loss: 0.425, Test Loss: 0.677\n",
      "Total Tokens seen till now: 108192\n",
      "Epoch No: 2, Step: 000195, Train Loss: 0.440, Test Loss: 0.658\n",
      "Total Tokens seen till now: 110816\n",
      "Epoch No: 2, Step: 000200, Train Loss: 0.389, Test Loss: 0.646\n",
      "Total Tokens seen till now: 113696\n",
      "Epoch No: 2, Step: 000205, Train Loss: 0.429, Test Loss: 0.640\n",
      "Total Tokens seen till now: 116216\n",
      "Epoch No: 2, Step: 000210, Train Loss: 0.401, Test Loss: 0.686\n",
      "Total Tokens seen till now: 119048\n",
      "Epoch No: 2, Step: 000215, Train Loss: 0.343, Test Loss: 0.663\n",
      "Total Tokens seen till now: 122104\n",
      "Epoch No: 2, Step: 000220, Train Loss: 0.385, Test Loss: 0.634\n",
      "Total Tokens seen till now: 125016\n",
      "Epoch No: 2, Step: 000225, Train Loss: 0.371, Test Loss: 0.682\n",
      "Total Tokens seen till now: 127976\n",
      "Epoch No: 2, Step: 000230, Train Loss: 0.421, Test Loss: 0.682\n",
      "Total Tokens seen till now: 130760\n",
      "-------------- Total Training Time : 57.91 minutes --------------\n"
     ]
    }
   ],
   "source": [
    "#Instruction Fine-Tuning of GPT2 355M Model:\n",
    "start_time = time.time()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device Available: ', device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt2_instruct.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "epochs = 2\n",
    "\n",
    "instruct_train = GPT2_PreTrain(model = gpt2_instruct, optimizer=optimizer, device=device, train_dataLoader= train_loader, test_dataLoader= val_loader, \n",
    "                               num_epochs=epochs, eval_batchSize= 5, eval_freq=5, start_context=input_text, max_new_tokens=100)\n",
    "\n",
    "train_losses, test_losses,  num_tokens_seen = instruct_train.train()\n",
    "\n",
    "end_time = time.time()\n",
    "train_time =(end_time - start_time) / 60\n",
    "print(f'-------------- Total Training Time : {train_time:.2f} minutes --------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction FineTuned Model saved at gpt2_355M_instruct_FineTuned.pth path.\n"
     ]
    }
   ],
   "source": [
    "#Save the instruct fine-tuned gpt2 355M model:\n",
    "model_fileName = model_name + '_instruct_FineTuned.pth'\n",
    "torch.save(gpt2_instruct.state_dict(),model_fileName)\n",
    "print('Instruction FineTuned Model saved at {0} path.'.format(model_fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6587565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, epochs, len(train_losses))\n",
    "plt = Plots(num_tokens_seen, epochs_tensor, train_losses, test_losses)\n",
    "plt.plots('loss', 'instruct_FineTune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77206e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Available:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Namrata Thakur\\AppData\\Local\\Temp\\ipykernel_10088\\307266843.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gpt2_instruct.load_state_dict(torch.load(model_fileName, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 1024)\n",
       "  (pos_embedding): Embedding(1024, 1024)\n",
       "  (token_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_block): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention_block): MultiHead_Attention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feedForward): FeedForwardBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_attention): LayerNormalization()\n",
       "      (layer_norm_feedforward): LayerNormalization()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_layerNorm): LayerNormalization()\n",
       "  (final_projection): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the instruct fine-tuned model:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device Available: ', device)\n",
    "model_fileName = model_name + '_instruct_FineTuned.pth'\n",
    "\n",
    "gpt2_instruct.load_state_dict(torch.load(model_fileName, map_location=device))\n",
    "gpt2_instruct.to(device)\n",
    "gpt2_instruct.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43cc7f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " What is the capital of Denmark?\n",
      "-----------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: What is the capital of Denmark?### Response:The capital of Denmark is Copenhagen.\n",
      "Generated Response ::  The capital of Denmark is Copenhagen.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "_, input_text = format_input_response(val_df[10], inference=True)\n",
    "print('Input Text:\\n',input_text)\n",
    "print('-----------------')\n",
    "\n",
    "generate = Text_Generation(model=gpt2_instruct, device=device, tokenizer_model='gpt2')\n",
    "output_text = generate.text_generation(input_text = input_text, max_new_tokens=35, temp=0.0,top_k= None, eos_id=50256)\n",
    "print(output_text)\n",
    "response = (output_text[len(input_text):]).replace(\"### Response:\", \" \").replace('Response:', '').strip()\n",
    "print('Generated Response :: ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_response(data, generate):\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        _, input_text = format_input_response(row, inference=True)\n",
    "        model_output = generate.text_generation(input_text = input_text, max_new_tokens=256, temp=0.0,top_k= None, eos_id=50256)\n",
    "        model_response = model_output[len(input_text):].replace(\"### Response:\", \"\").replace('Response:', '').strip()\n",
    "        data[i]['model_response'] = model_response\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae42b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model responses of the test data:\n",
    "generate = Text_Generation(model=gpt2_instruct, device=device, tokenizer_model='gpt2')\n",
    "test_data_response = save_model_response(test_df, generate)\n",
    "\n",
    "print('Example Test Data with model response::\\n',test_data_response[0])\n",
    "\n",
    "with open('gpt2_355M_instructionData_response.json', \"w\") as file:\n",
    "    json.dump(test_data_response, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a86d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Explain the primary function of the human heart.', 'input': '', 'output': 'The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.', 'model_response': 'The primary function of the human heart is to pump blood to the brain and to supply oxygen to the body.'}\n"
     ]
    }
   ],
   "source": [
    "with open('gpt2_355M_instructionData_response.json', \"r\") as file:\n",
    "    test_data_response = json.load(file)\n",
    "\n",
    "print(test_data_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15fc9bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT:  The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
      "Generated:  The primary function of the human heart is to pump blood to the brain and to supply oxygen to the body.\n",
      "------------------------------------\n",
      "GT:  He will be reading a novel inspired by his grandmother.\n",
      "Generated:  He is reading a novel inspired by his grandmother.\n",
      "------------------------------------\n",
      "GT:  The government passed the law.\n",
      "Generated:  The law was passed by the government.\n",
      "------------------------------------\n",
      "GT:  The confrontation was inevitable given the circumstances.\n",
      "Generated:  The decision was inevitable.\n",
      "------------------------------------\n",
      "GT:  Opinion-based.\n",
      "Generated:  The chocolate is the best dessert.\n",
      "------------------------------------\n",
      "GT:  young.\n",
      "Generated:  An antonym of 'old' is 'young'.\n",
      "------------------------------------\n",
      "GT:  A synonym for 'hardworking' is 'diligent'.\n",
      "Generated:  A synonym for 'hardworking' is 'courageous'.\n",
      "------------------------------------\n",
      "GT:  The boiling point of sulfur is 444.6 degrees Celsius.\n",
      "Generated:  The boiling point of sulfur is -114.5 degrees Celsius.\n",
      "------------------------------------\n",
      "GT:  The plural form of 'child' is 'children'.\n",
      "Generated:  The plural form of 'child' is 'children'.\n",
      "------------------------------------\n",
      "GT:  An antonym of 'complicated' is 'simple'.\n",
      "Generated:  An antonym of 'complicated' is 'easy'.\n",
      "------------------------------------\n",
      "GT:  The three forms of water are solid (ice), liquid (water), and gas (steam).\n",
      "Generated:  Three forms of water are rain, snow, and sleet.\n",
      "------------------------------------\n",
      "GT:  Did the dog chase the cat?\n",
      "Generated:  The cat chased the dog.\n",
      "------------------------------------\n",
      "GT:  The movie was long. It was interesting.\n",
      "Generated:  d 'The movie was a success.'The movie was long and interesting.\n",
      "------------------------------------\n",
      "GT:  Acid: Lemon juice\n",
      "Base: Soap\n",
      "Neutral: Water\n",
      "Generated:  AcidBaseNeutral\n",
      "------------------------------------\n",
      "GT:  A synonym for 'sad' is 'unhappy'.\n",
      "Generated:  A synonym for 'sad' is 'determined'.\n",
      "------------------------------------\n",
      "GT:  I prefer homemade cookies to store bought.\n",
      "Generated:  I prefer homemade cookies to store boaght.\n",
      "------------------------------------\n",
      "GT:  His stay in the city was transient, lasting only a couple of days.\n",
      "Generated:  He is very quick.\n",
      "------------------------------------\n",
      "GT:  The Italian translation of 'I am lost' is 'Mi sono perso' (if male) or 'Mi sono persa' (if female).\n",
      "Generated:  Il pi amo.\n",
      "------------------------------------\n",
      "GT:  Technical document\n",
      "Generated:  Technical document.\n",
      "------------------------------------\n",
      "GT:  25, 16, 10, 7, 2.\n",
      "Generated:  The list is in descending order.\n",
      "------------------------------------\n",
      "GT:  Puis-je avoir de l'eau?\n",
      "Generated:  The French translation of 'Can I have some water?' is 'Je suis a va?'.\n",
      "------------------------------------\n",
      "GT:  Her hands were as cold as ice.\n",
      "Generated:  The simile is as cold as ice.\n",
      "------------------------------------\n",
      "GT:  Swim: Verb\n",
      "Beautiful: Adjective\n",
      "Quickly: Adverb\n",
      "Generated:  # Input: The swimmer was quick.The swimmer was quick.\n",
      "------------------------------------\n",
      "GT:  The density of the object is 3 grams per cubic centimeter.\n",
      "Generated:  The density of the object is 15 grams/cm.\n",
      "------------------------------------\n",
      "GT:  The abbreviation for 'Master of Business Administration' is MBA.\n",
      "Generated:  The abbreviation for 'Master of Business Administration' is MBA.\n",
      "------------------------------------\n",
      "GT:  The number IX in Roman numerals is 9.\n",
      "Generated:  o decimal: 9.9.\n",
      "------------------------------------\n",
      "GT:  The opposite of 'horizontal' is 'vertical'.\n",
      "Generated:  The opposite of 'horizontal' is 'vertical'.\n",
      "------------------------------------\n",
      "GT:  The Italian translation for 'Where can I buy tickets?' is 'Dove posso comprare i biglietti?'\n",
      "Generated:  Il pi vicino?\n",
      "------------------------------------\n",
      "GT:  He remained very calm.\n",
      "Generated:  He was as cool as a cucumber.\n",
      "------------------------------------\n",
      "GT:  The main verb in the sentence is 'barked'.\n",
      "Generated:  The main verb in the sentence is 'bark.'\n",
      "------------------------------------\n",
      "GT:  The professor attempted to elucidate the complex topic for his students.\n",
      "Generated:  He was very quick.\n",
      "------------------------------------\n",
      "GT:  My friend and I went to the store.\n",
      "Generated:  The store was very crowded.\n",
      "------------------------------------\n",
      "GT:  The formula for calculating work done is work = force  distance.\n",
      "Generated:  The formula for calculating work done is the product of the product of the two product of the two numbers.\n",
      "------------------------------------\n",
      "GT:  The chemical formula for ammonium nitrate is NH4NO3.\n",
      "Generated:  The chemical formula for ammonium nitrate is NH3NO.\n",
      "------------------------------------\n",
      "GT:  The molecular formula for water is H2O.\n",
      "Generated:  The molecular formula for water is H2O.\n",
      "------------------------------------\n",
      "GT:  The food could use some improvement.\n",
      "Generated:  The food was delicious.\n",
      "------------------------------------\n",
      "GT:  The opposite of 'lazy' is 'diligent'.\n",
      "Generated:  The opposite of 'lazy' is 'enthusiastic'.\n",
      "------------------------------------\n",
      "GT:  1. Vitamin A\n",
      "2. Vitamin C\n",
      "3. Vitamin D\n",
      "Generated:  1. Vitamin B122. Vitamin C3. Vitamin B6\n",
      "------------------------------------\n",
      "GT:  A simile is a figure of speech that directly compares two different things, often introduced with the words 'like' or 'as'.\n",
      "Generated:  A simile is a figurative or simile that simulates the sound of a voice.\n",
      "------------------------------------\n",
      "GT:  The boiling point of chlorine is -34 degrees Celsius.\n",
      "Generated:  The boiling point of chlorine is -183.15 degrees Celsius.\n",
      "------------------------------------\n",
      "GT:  The French translation of 'My name is' is 'Je m'appelle'.\n",
      "Generated:  The French translation of 'My name is' is 'Je suis heureuse'.\n",
      "------------------------------------\n",
      "GT:  200 centimeters is 2 meters.\n",
      "Generated:  200 centimeters is 0.2 meters.\n",
      "------------------------------------\n",
      "GT:  Zn.\n",
      "Generated:  The chemical symbol for zinc is Zn.\n",
      "------------------------------------\n",
      "GT:  The formula for calculating force is Force = mass x acceleration.\n",
      "Generated:  The formula for calculating force is the square of the force.\n",
      "------------------------------------\n",
      "GT:  The company's innovative approach set it apart from its competitors.\n",
      "Generated:  The team was innovative and innovative ideas were implemented.\n",
      "------------------------------------\n",
      "GT:  He is very generous and always helps those in need.\n",
      "Generated:  He was very generous.\n",
      "------------------------------------\n",
      "GT:  An antonym of 'sharp' is 'dull'.\n",
      "Generated:  An antonym of 'sharp' is 'soft'.\n",
      "------------------------------------\n",
      "GT:  A neuron consists of three main parts: the cell body, which contains the nucleus; dendrites, which receive signals from other neurons; and an axon, which transmits signals to other neurons, muscles, or glands.\n",
      "Generated:  A neuron is a type of synapse. It contains a synapse that is either a synapse or a synapse that is a synapse.\n",
      "------------------------------------\n",
      "GT:  Hasta luego\n",
      "Generated:  Estn leyendo.\n",
      "------------------------------------\n",
      "GT:  0, 1, 1, 2, 3, 5, 8, 13, 21, 34.\n",
      "Generated:  The first 10 elements of the Fibonacci sequence are 9, 15, 21, 25, 35, 44, 56, 64.\n",
      "------------------------------------\n",
      "GT:  An antonym of 'transparent' is 'opaque'.\n",
      "Generated:  An antonym of 'transparent' is 'illuminate'.\n",
      "------------------------------------\n",
      "GT:  The past tense of 'think' is 'thought'.\n",
      "Generated:  The past tense of 'think' is 'conceal'.\n",
      "------------------------------------\n",
      "GT:  The classification of the sentence 'Please open the door.' is imperative.\n",
      "Generated:  The sentence 'Please open the door' is interrogative.\n",
      "------------------------------------\n",
      "GT:  She never forgets to call.\n",
      "Generated:  She always misses her calls.\n",
      "------------------------------------\n",
      "GT:  50 miles per hour is approximately 80.47 kilometers per hour.\n",
      "Generated:  50 miles per hour is approximately 4.5 kilometers per hour.\n",
      "------------------------------------\n",
      "GT:  The meal is cooked by the chef every day.\n",
      "Generated:  The meal is cooked every day by the chef.\n",
      "------------------------------------\n",
      "GT:  'Dance' can be classified as a verb.\n",
      "Generated:  Danced.\n",
      "------------------------------------\n",
      "GT:  The book is a page-turner.\n",
      "Generated:  The book is a treat.\n",
      "------------------------------------\n",
      "GT:  How do bacteria affect human health?\n",
      "Generated:  # Bacteria is a type of living organism.\n",
      "------------------------------------\n",
      "GT:  The past participle of 'run' is 'run'.\n",
      "Generated:  The past participle of 'run' is 'rushed'.\n",
      "------------------------------------\n",
      "GT:  A synonym for 'quick' is 'rapid'.\n",
      "Generated:  A synonym for 'quick' is 'quick'.\n",
      "------------------------------------\n",
      "GT:  An example of a metaphor is saying, 'Time is a thief,' which suggests that time, like a thief, can take away life moments without literally stealing.\n",
      "Generated:  The metaphor is the passage of time.\n",
      "------------------------------------\n",
      "GT:  The interest on a $1,000 investment at 5% annual rate over 3 years is $150.\n",
      "Generated:  The annual rate of 5% is approximately $1,000.\n",
      "------------------------------------\n",
      "GT:  A synonym for 'elated' is 'overjoyed'.\n",
      "Generated:  A synonym for 'elated' is 'joyful'.\n",
      "------------------------------------\n",
      "GT:  The Japanese translation of 'It's raining' is '' (Ame ga futte imasu).\n",
      "Generated:  The Japanese translation of 'It's raining' is 'Ksmo desu ka'.\n",
      "------------------------------------\n",
      "GT:  The past participle form of 'speak' is 'spoken.'\n",
      "Generated:  The past participle form of 'speak' is 'quoted'.\n",
      "------------------------------------\n",
      "GT:  The principle of conservation of energy states that energy cannot be created or destroyed, only transformed from one form to another. This means the total energy of an isolated system remains constant over time.\n",
      "Generated:  The principle of conservation of energy is that the energy stored in a substance cannot be converted into kinetic energy.\n",
      "------------------------------------\n",
      "GT:  The perimeter of the rectangle is 16 meters.\n",
      "Generated:  The perimeter of the rectangle is approximately 25 meters.\n",
      "------------------------------------\n",
      "GT:  The past tense of 'freeze' is 'froze'.\n",
      "Generated:  The past tense of 'freeze' is 'frozen'.\n",
      "------------------------------------\n",
      "GT:  The corrected sentence should be: 'They're going to the store.'\n",
      "Generated:  The misspelled 'their' was corrected by the correct spelling.\n",
      "------------------------------------\n",
      "GT:  The statement \"My computer crashed\" can be classified as negative.\n",
      "Generated:  The statement 'My computer crashed' is classified as positive.\n",
      "------------------------------------\n",
      "GT:  The onomatopoeia in the sentence is 'buzzed'.\n",
      "Generated:  The onomatopoeia in the sentence is the sound of the wind.\n",
      "------------------------------------\n",
      "GT:  We enjoy watching movies.\n",
      "Generated:  We enjoy watching movies.\n",
      "------------------------------------\n",
      "GT:  Gravity is the force that attracts a body toward the center of the earth, or toward any other physical body having mass.\n",
      "Generated:  Gravity is the force that attracts two bodies toward each other.\n",
      "------------------------------------\n",
      "GT:  An antonym for 'hot' is 'cold'.\n",
      "Generated:  An antonym for 'hot' is 'cold'.\n",
      "------------------------------------\n",
      "GT:  moon and sun\n",
      "Generated:  The moon and sun were at the same time at the same place.\n",
      "------------------------------------\n",
      "GT:  The formula for calculating power is power = work/time.\n",
      "Generated:  The formula for calculating power is the square root of the base.\n",
      "------------------------------------\n",
      "GT:  The process by which plants absorb water through their roots is called absorption.\n",
      "Generated:  The process by which plants absorb water through their roots is called photosynthesis.\n",
      "------------------------------------\n",
      "GT:  drew.\n",
      "Generated:  The past tense of 'draw' is 'drew.'\n",
      "------------------------------------\n",
      "GT:  Run.\n",
      "Generated:  The present perfect form of the verb 'run' is 'flew'.\n",
      "------------------------------------\n",
      "GT:  An oxymoron is a figure of speech in which apparently contradictory terms appear in conjunction.\n",
      "Generated:  Oxymoron is a sentence that is neither logically nor logically connected.\n",
      "------------------------------------\n",
      "GT:  The chemical symbol for water is H2O.\n",
      "Generated:  The chemical symbol for water is H2O.\n",
      "------------------------------------\n",
      "GT:  The house is painted every year by them.\n",
      "Generated:  They paint the house every year.\n",
      "------------------------------------\n",
      "GT:  The past tense of the verb \"run\" is \"ran.\"\n",
      "Generated:  The past tense of the verb \"run\" is \"rushed.\"\n",
      "------------------------------------\n",
      "GT:  The meeting will be held tomorrow by them.\n",
      "Generated:  They will hold the meeting tomorrow.\n",
      "------------------------------------\n",
      "GT:  The chemical formula for sulfuric acid is H2SO4.\n",
      "Generated:  The chemical formula for sulfuric acid is H2SO4.\n",
      "------------------------------------\n",
      "GT:  Cumulus clouds often indicate fair weather.\n",
      "Generated:  A cloud with a temperature of 25 degrees Celsius or lower is typically a fair weather cloud.\n",
      "------------------------------------\n",
      "GT:  The journey was arduous.\n",
      "Generated:  The journey was a journey.\n",
      "------------------------------------\n",
      "GT:  The test was a piece of cake.\n",
      "Generated:  The test was a breeze.\n",
      "------------------------------------\n",
      "GT:  The opposite of 'increase' is 'decrease'.\n",
      "Generated:  The opposite of 'increase' is 'decrease'.\n",
      "------------------------------------\n",
      "GT:  The perimeter of the square is 24 meters.\n",
      "Generated:  The perimeter of the square is 24 meters.\n",
      "------------------------------------\n",
      "GT:  The missing number in the sequence is 6.\n",
      "Generated:  The missing number in the sequence is 8.\n",
      "------------------------------------\n",
      "GT:  The German translation of 'Good night' is 'Gute Nacht'.\n",
      "Generated:  The German translation of 'Good night' is 'Das ist es'.\n",
      "------------------------------------\n",
      "GT:  A word that rhymes with 'light' is 'might.'\n",
      "Generated:  A light.\n",
      "------------------------------------\n",
      "GT:  The comparative form of 'good' is 'better'.\n",
      "Generated:  The comparative form of 'good' is 'good' or 'excellent'.\n",
      "------------------------------------\n",
      "GT:  The literary device used is personification.\n",
      "Generated:  The literary device used is metaphor.\n",
      "------------------------------------\n",
      "GT:  His words were a knife, cutting deep.\n",
      "Generated:  His words cut deeper than a knife.\n",
      "------------------------------------\n",
      "GT:  The company aims to innovate by developing new and sustainable technologies.\n",
      "Generated:  He is very innovative.\n",
      "------------------------------------\n",
      "GT:  The speed of light is approximately 299,792 kilometers per second.\n",
      "Generated:  The speed of light is 186,792,458 meters per second.\n",
      "------------------------------------\n",
      "GT:  The adverb in the sentence is 'quickly'.\n",
      "Generated:  The adverb in the sentence is 'quickly'.\n",
      "------------------------------------\n",
      "GT:  A synonym for 'beautiful' is 'gorgeous'.\n",
      "Generated:  A synonym for 'beautiful' is 'curious'.\n",
      "------------------------------------\n",
      "GT:  The capital of India is New Delhi.\n",
      "Generated:  The capital of India is New Delhi.\n",
      "------------------------------------\n",
      "GT:  Jump: Verb\n",
      "Quick: Adjective\n",
      "Beautiful: Adjective\n",
      "Generated:  # Input: The quick is a verb.The quick is a noun.\n",
      "------------------------------------\n",
      "GT:  The chemical formula for potassium nitrate is KNO3.\n",
      "Generated:  The chemical formula for potassium nitrate is KNO3.\n",
      "------------------------------------\n",
      "GT:  The cake is as sweet as honey.\n",
      "Generated:  The cake is as sweet as honey.\n",
      "------------------------------------\n",
      "GT:  False\n",
      "Generated:  The moon is a planet.\n",
      "------------------------------------\n",
      "GT:  1000 milliliters is equivalent to 1 liter.\n",
      "Generated:  1000 milliliters is 1000 milliliters.\n",
      "------------------------------------\n",
      "GT:  She was singing in the choir.\n",
      "Generated:  She sang in the choir.\n",
      "------------------------------------\n",
      "GT:  A synonym for 'clever' is 'smart.'\n",
      "Generated:  A synonym for 'clever' is 'smart.'\n",
      "------------------------------------\n",
      "GT:  Should you have called me?\n",
      "Generated:  You should have called me.\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for row in test_data_response:\n",
    "    print('GT: ', row['output'])\n",
    "    print('Generated: ', row['model_response'].replace('sponse:',''))\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa5ca1",
   "metadata": {},
   "source": [
    "#### OLLAMA Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c38e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "#The below three functions are taken as in from ch07.ipynb file of 'Build LLM from Scratch' Book by SR:\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe8ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and strong bones.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, like willow or cedar.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or birch.\n",
      "3. Mosses: Llamas have been known to graze on mosses and other types of non-woody plants.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42173ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 40.71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(data):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input_response(entry, inference=True)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key].replace('sponse:','')}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data_response, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data_response)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb12a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_dict = {\n",
    "    model_fileName : sum(scores)/len(scores)\n",
    "}\n",
    "\n",
    "with open('Model_Scores_Record.json', 'w') as file:\n",
    "    json.dump(model_score_dict, file, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d16c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c754bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a9c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf0b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da68df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e239ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b60a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5cfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93159b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-cuda12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
