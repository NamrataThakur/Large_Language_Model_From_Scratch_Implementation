{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-on Code Implementation of the Chapter 03 of 'Build LLM From Scratch'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1704db4c0f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from importlib.metadata import version\n",
    "import random\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example Input Tensor:\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Type 1: Self-Attention without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an input tensor (input embeddings) of \"m\" dimension, output context vector of size \"n\" dimension (m >,<,= n)\n",
    "\n",
    "#Step 1: Attention Score\n",
    "def attention_score(input_tensor,all=True):\n",
    "\n",
    "    if all:\n",
    "        print('Computing attention scores for all inputs: ')\n",
    "        score = torch.zeros(input_tensor.shape)\n",
    "        score = input_tensor @ input_tensor.T\n",
    "        \n",
    "        return score\n",
    "    else:\n",
    "        print('Computing attention scores for 1 input: ')\n",
    "        query = random.choice(list(range(input_tensor.shape[0])))\n",
    "        print('Randomly selected Query vector : Input ', query)\n",
    "        q_vector = input_tensor[query]\n",
    "        score = torch.empty(input_tensor.shape[0])\n",
    "        for i, x_i in enumerate(input_tensor):\n",
    "            score[i] = torch.dot(x_i,q_vector)\n",
    "    \n",
    "        return score, query\n",
    "\n",
    "#Step 2: Attention weights\n",
    "def attention_weight(score):\n",
    "    \n",
    "    print(\"Computing normalized attention weights: \")\n",
    "    weights = torch.zeros(score.shape)\n",
    "    weights = torch.softmax(score,dim=-1)\n",
    "    print('SUM : ', weights.sum(dim=-1))\n",
    "    print('Attention Weights: \\n',weights)\n",
    "    return weights\n",
    "\n",
    "#Step 3: Context vectors\n",
    "def context_vector(input_tensor,all=True):\n",
    "    \n",
    "    if all:\n",
    "        att_score = attention_score(input_tensor,all)\n",
    "        att_weight = attention_weight(att_score)\n",
    "        print('Computing context vectors for ALL input: ')\n",
    "        context = att_weight @ input_tensor\n",
    "    \n",
    "    else:\n",
    "        att_score,query = attention_score(input_tensor,all)\n",
    "        att_weight = attention_weight(att_score)\n",
    "        print('Computing context vector for input: ', query)\n",
    "        \n",
    "        context = torch.zeros(input_tensor[query].shape)\n",
    "        for i, x_i in enumerate(input_tensor):\n",
    "            context += att_weight[i] * x_i\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing attention scores for all inputs: \n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "Attention Weights: \n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "Computing context vectors for ALL input: \n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "*********************\n",
      "\n",
      "Computing attention scores for 1 input: \n",
      "Randomly selected Query vector : Input  2\n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor(1.0000)\n",
      "Attention Weights: \n",
      " tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n",
      "Computing context vector for input:  2\n",
      "tensor([0.4431, 0.6496, 0.5671])\n"
     ]
    }
   ],
   "source": [
    "## TESTING:: \n",
    "context_all = context_vector(inputs,True)\n",
    "print(context_all)\n",
    "print('*********************\\n')\n",
    "context_1 = context_vector(inputs,False)\n",
    "print(context_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Type 2: Self-Attention with trainable weights (SCALED DOT PRODUCT ATTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an input tensor (input embeddings) of \"m\" dimension, output context vector of size \"n\" dimension (m >,<,= n)\n",
    "\n",
    "#Step 1: Define the 3 weight matrices: Q,K,V (add the bias parameter if using torch.nn.Linear ONLY)\n",
    "def create_attention_matrices(dim_in, dim_out,qkv_bias=False):\n",
    "\n",
    "    #USING torch.nn.Parameter:\n",
    "    # W_query = torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=True)\n",
    "    # W_key = torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=True)\n",
    "    # W_value = torch.nn.Parameter(torch.rand(dim_in,dim_out),requires_grad=True)\n",
    "\n",
    "    #USING torch.nn.Linear:\n",
    "    W_query = torch.nn.Linear(dim_in,dim_out,bias=qkv_bias)\n",
    "    W_key = torch.nn.Linear(dim_in,dim_out,bias=qkv_bias)\n",
    "    W_value = torch.nn.Linear(dim_in,dim_out,bias=qkv_bias)\n",
    "\n",
    "    return W_query, W_key, W_value\n",
    "\n",
    "#Step 2: Project the input tensor through the 3 matrices to get the 3 vectors: query_vec, key_vector, value_vector\n",
    "def create_input_vectors(input_tensor, dim_out=2,qkv_bias=False):\n",
    "\n",
    "    dim_in = input_tensor.shape[-1]\n",
    "    W_query,W_key, W_value = create_attention_matrices(dim_in,dim_out,qkv_bias=False)\n",
    "\n",
    "    #USING torch.nn.Parameter:\n",
    "    # Vec_query = input_tensor @ W_query\n",
    "    # Vec_key = input_tensor @ W_key\n",
    "    # Vec_value = input_tensor @ W_value\n",
    "\n",
    "    #USING torch.nn.Linear:\n",
    "    Vec_query = W_query(input_tensor)\n",
    "    Vec_key = W_key(input_tensor)\n",
    "    Vec_value = W_value(input_tensor)\n",
    "\n",
    "    return Vec_query, Vec_key, Vec_value\n",
    "\n",
    "#Step 3: Compute attention scores using query_vector and key_vector\n",
    "def self_attention_score(input_tensor, dim_out=2,all=True,qkv_bias=False):\n",
    "    \n",
    "    Vec_query, Vec_key, Vec_value = create_input_vectors(input_tensor,dim_out,qkv_bias=False)\n",
    "    \n",
    "    if all:\n",
    "        print('Computing attention scores for ALL inputs: ')\n",
    "        att_score = Vec_query @ Vec_key.T\n",
    "        \n",
    "    else:\n",
    "        print('Computing attention scores for 1 input: ')\n",
    "        query = random.choice(list(range(input_tensor.shape[0])))\n",
    "        print('Randomly selected Query vector : Input ', query)\n",
    "        att_score = Vec_query[query] @ Vec_key.T\n",
    "    \n",
    "    return att_score, Vec_value\n",
    "\n",
    "#Step 4: Normalize attention scores to get attention weights\n",
    "def self_attention_weight(score,dim_k):\n",
    "    print(\"Computing normalized attention weights: \")\n",
    "    weights = torch.softmax(score /dim_k**0.5, dim=-1)\n",
    "    print('SUM : ', weights.sum(dim=-1))\n",
    "    print('Attention Weights: \\n',weights)\n",
    "    return weights\n",
    "\n",
    "#Step 5: Compute context vectors using key_vector and attention weights\n",
    "def context_vector(input_tensor,dim_out=2, all= True,qkv_bias=False):\n",
    "    \n",
    "    att_score, Vec_value = self_attention_score(input_tensor,dim_out,all,qkv_bias=False)\n",
    "    dim_k = Vec_value.shape[-1]\n",
    "    att_weight = self_attention_weight(att_score,dim_k)\n",
    "    context = att_weight @ Vec_value\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing attention scores for ALL inputs: \n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: \n",
      " tensor([[0.1640, 0.1621, 0.1618, 0.1738, 0.1626, 0.1758],\n",
      "        [0.1699, 0.1542, 0.1545, 0.1765, 0.1732, 0.1716],\n",
      "        [0.1699, 0.1543, 0.1546, 0.1765, 0.1730, 0.1717],\n",
      "        [0.1691, 0.1592, 0.1594, 0.1719, 0.1723, 0.1681],\n",
      "        [0.1694, 0.1599, 0.1598, 0.1726, 0.1663, 0.1720],\n",
      "        [0.1690, 0.1576, 0.1580, 0.1728, 0.1751, 0.1675]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[ 0.5029,  0.3415, -0.5182, -0.2100,  0.2738],\n",
      "        [ 0.4999,  0.3371, -0.5156, -0.2072,  0.2678],\n",
      "        [ 0.4999,  0.3372, -0.5157, -0.2073,  0.2679],\n",
      "        [ 0.5027,  0.3390, -0.5186, -0.2080,  0.2690],\n",
      "        [ 0.5025,  0.3394, -0.5185, -0.2093,  0.2707],\n",
      "        [ 0.5021,  0.3384, -0.5178, -0.2072,  0.2680]], grad_fn=<MmBackward0>)\n",
      "*********************\n",
      "\n",
      "Computing attention scores for 1 input: \n",
      "Randomly selected Query vector : Input  3\n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor(1., grad_fn=<SumBackward1>)\n",
      "Attention Weights: \n",
      " tensor([0.1703, 0.1551, 0.1554, 0.1749, 0.1753, 0.1690],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([-0.1531,  0.3582,  0.0456, -0.4446,  0.4931],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "## TESTING:: \n",
    "context_all = context_vector(inputs,5,True,qkv_bias=False)\n",
    "print(context_all)\n",
    "print('*********************\\n')\n",
    "context_1 = context_vector(inputs,5,False,qkv_bias=False)\n",
    "print(context_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Type 3: Causal (Masked) and Sparse Self-Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an input tensor (input embeddings) of \"m\" dimension, output context vector of size \"n\" dimension (m >,<,= n)\n",
    "#Build upon the previous self-attention scores and weights calculation:    \n",
    "    \n",
    "#Step 2: Depending upon normalization choice (softmax/regular), create the mask\n",
    "def create_causal_mask(context_length,softmax=True):\n",
    "    \n",
    "    mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "    return mask\n",
    "\n",
    "#Step 3: Apply mask on self-attention scores or weights to get masked weights\n",
    "def causal_attention_weights(score_weight,context_length,softmax):\n",
    "    \n",
    "    mask = create_causal_mask(context_length,softmax)\n",
    "    \n",
    "    if softmax:\n",
    "        masked_scores = score_weight.masked_fill(mask.bool(), -torch.inf)\n",
    "    else:\n",
    "        masked_scores = score_weight.masked_fill(mask.bool(),0)\n",
    "    \n",
    "    return masked_scores\n",
    "\n",
    "#Step 6: Compute context vectors using key_vector and (sparse + masked) attention weights\n",
    "def sparseCausal_context_vector(input_tensor,dim_out,all,softmax,qkv_bias=False):\n",
    "\n",
    "    #Step 1: Load the self-attention scores and weights\n",
    "    score_weight, Vec_value = self_attention_score(input_tensor,dim_out,all,qkv_bias)\n",
    "        \n",
    "    if not softmax:    \n",
    "        dim_k = Vec_value.shape[-1]\n",
    "        score_weight = self_attention_weight(score_weight,dim_k)\n",
    "        \n",
    "    context_length = input_tensor.shape[0]\n",
    "    masked_scores = causal_attention_weights(score_weight,context_length,softmax)\n",
    "    print('Masked Scores/Weights : \\n', masked_scores)\n",
    "\n",
    "    #Step 4: Normalize to get masked causal weights\n",
    "    dim_k = Vec_value.shape[-1]\n",
    "    \n",
    "    if softmax:\n",
    "        masked_weights = self_attention_weight(masked_scores,dim_k)\n",
    "    else:\n",
    "        row_sum = masked_scores.sum(dim=-1,keepdim=True)\n",
    "        masked_weights = masked_scores/row_sum\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    #Step 5: Add Dropout to create sparcity in the masked causal weight matrix\n",
    "    dropout = torch.nn.Dropout(0.5)\n",
    "    sparse_maskedWeights = dropout(masked_weights)\n",
    "    print('Dropout Weights: \\n',sparse_maskedWeights )\n",
    "    context = sparse_maskedWeights @ Vec_value\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing attention scores for ALL inputs: \n",
      "Masked Scores/Weights : \n",
      " tensor([[-0.2541,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.4388, -0.6557,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.4341, -0.6496, -0.6458,    -inf,    -inf,    -inf],\n",
      "        [-0.2486, -0.3847, -0.3811, -0.2127,    -inf,    -inf],\n",
      "        [-0.2277, -0.3571, -0.3577, -0.1863, -0.2684,    -inf],\n",
      "        [-0.3094, -0.4661, -0.4605, -0.2595, -0.2306, -0.3302]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5242, 0.4758, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3549, 0.3223, 0.3228, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2564, 0.2413, 0.2417, 0.2606, 0.0000, 0.0000],\n",
      "        [0.2046, 0.1931, 0.1930, 0.2084, 0.2009, 0.0000],\n",
      "        [0.1690, 0.1576, 0.1580, 0.1728, 0.1751, 0.1675]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Dropout Weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9515, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6457, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5129, 0.4826, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4092, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3152, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.9544,  0.2127, -1.1934, -0.5507,  0.0379],\n",
      "        [ 0.6442,  0.4738, -0.6480, -0.2637,  0.3967],\n",
      "        [ 0.4367,  0.3194, -0.4386, -0.1731,  0.2621],\n",
      "        [ 0.5714,  0.2949, -0.6347, -0.2750,  0.2109],\n",
      "        [ 0.1953,  0.0435, -0.2442, -0.1127,  0.0078],\n",
      "        [ 0.2134,  0.1570, -0.2147, -0.0874,  0.1314]], grad_fn=<MmBackward0>)\n",
      "*********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TESTING:: \n",
    "torch.manual_seed(123)\n",
    "context_all = sparseCausal_context_vector(inputs,5,True,True,qkv_bias=False)\n",
    "print(context_all)\n",
    "print('*********************\\n')\n",
    "# context_1 = sparseCausal_context_vector(inputs,5,True,False)\n",
    "# print(context_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Type 4: Multi Head Causal Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VERSION 1: Stacking multiple single-head attention layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an input tensor (input embeddings) of \"m\" dimension, output context vector of size \"n\" dimension (m >,<,= n)\n",
    "#Build upon the previous causal self-attention scores and weights calculation:   \n",
    "\n",
    "def multiHead_context_vectors(input_tensor,heads,dim_out,all,softmax,qkv_bias=False):\n",
    "\n",
    "    mha_context_vectors = []\n",
    "\n",
    "    #Step 1: Get the different context vectors from each head:\n",
    "    for i in range(heads):\n",
    "        print('Head: ', i)\n",
    "        context_vector = sparseCausal_context_vector(input_tensor,dim_out,all,softmax,qkv_bias=False)\n",
    "        mha_context_vectors.append(context_vector)\n",
    "\n",
    "        #Step 2: Concatenate all the heads output to create the final context vector:\n",
    "        mha = torch.cat([i for i in mha_context_vectors],dim=-1)\n",
    "    \n",
    "    assert mha.shape[-1] == heads * dim_out\n",
    "    return mha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:  0\n",
      "Computing attention scores for ALL inputs: \n",
      "Masked Scores/Weights : \n",
      " tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Dropout Weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Head:  1\n",
      "Computing attention scores for ALL inputs: \n",
      "Masked Scores/Weights : \n",
      " tensor([[ 0.2553,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0907,  0.0832,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0956,  0.0850,  0.0854,    -inf,    -inf,    -inf],\n",
      "        [ 0.0024,  0.0362,  0.0368,  0.0205,    -inf,    -inf],\n",
      "        [ 0.1567,  0.0936,  0.0935,  0.0342,  0.0649,    -inf],\n",
      "        [-0.0391,  0.0222,  0.0230,  0.0177,  0.0322,  0.0097]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Computing normalized attention weights: \n",
      "SUM :  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5013, 0.4987, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3350, 0.3325, 0.3326, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2462, 0.2522, 0.2523, 0.2494, 0.0000, 0.0000],\n",
      "        [0.2098, 0.2006, 0.2006, 0.1924, 0.1966, 0.0000],\n",
      "        [0.1609, 0.1680, 0.1681, 0.1674, 0.1692, 0.1665]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Dropout Weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9973, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6651, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4924, 0.5043, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3359, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[-0.9038,  0.4432, -0.5668, -0.5077],\n",
      "        [-0.7381, -0.2026, -0.4508, -0.0032],\n",
      "        [-0.4849, -0.1341, -0.3010, -0.0112],\n",
      "        [-0.5845,  0.0085, -0.3675, -0.1266],\n",
      "        [-0.1802,  0.0884, -0.1189, -0.1065],\n",
      "        [-0.2441, -0.0670, -0.1518, -0.0011]], grad_fn=<CatBackward0>)\n",
      "MHA Context Vector Shape:  torch.Size([6, 4])\n",
      "*********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TESTING:: \n",
    "torch.manual_seed(123)\n",
    "heads = 2\n",
    "dim_out = 2\n",
    "context_all = multiHead_context_vectors(inputs,heads,dim_out,True,True,qkv_bias=False)\n",
    "print(context_all)\n",
    "print('MHA Context Vector Shape: ', context_all.shape)\n",
    "print('*********************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VERSION 2: Multi head attention layers with weight splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an input tensor (input embeddings) of \"m\" dimension, output context vector of size \"n\" dimension (m >,<,= n)\n",
    "#Build upon the previous causal self-attention scores and weights calculation: \n",
    "\n",
    "def mha_causal_attention(input_tensor, heads, dim_out,dropout_rate, qkv_bias=False):\n",
    "\n",
    "    #Step 1: Get the necessary details from the input tensor:\n",
    "    batch, context_length, dim_in = input_tensor.shape #Shape: batch_size, 6,3\n",
    "\n",
    "    #Step 2: Make sure the final output dimension and the number of heads are situable for attention operation:\n",
    "    assert (dim_out % heads) == 0, \\\n",
    "            \"d_out must be divisible by heads\"\n",
    "    \n",
    "    #Step 3: Calculate the output dimension of each heads:\n",
    "    dim_head = dim_out // heads\n",
    "    print('Dimension of each heads: ', dim_head)\n",
    "    print('Context length: ', context_length)\n",
    "    print('Input embedding dimension detected: ', dim_in)\n",
    "    print('Final context vector dimension (required): ', dim_out)\n",
    "\n",
    "    #Step 4: Create the Q,K and V matrices:\n",
    "    W_query = torch.nn.Linear(dim_in,dim_out,bias=qkv_bias) #Shape : 3x4\n",
    "    W_key = torch.nn.Linear(dim_in,dim_out,bias=qkv_bias)\n",
    "    W_value = torch.nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "    \n",
    "    #Step 5: Get the Q,K, and V projections\n",
    "    Vec_query = W_query(input_tensor) #Shape: batch_size, 6,4\n",
    "    Vec_key = W_key(input_tensor)\n",
    "    Vec_value = W_value(input_tensor)\n",
    "\n",
    "    print('Vec_query Shape before unrolling: ',Vec_query.shape) \n",
    "    \n",
    "    #Step 6: Divide the original Q,K,V projections into smaller projections (each projection for each head). Attention will be computed on each of these smaller projections.\n",
    "    Vec_query = Vec_query.view(batch,context_length, heads, dim_head) #Shape: b,6,2,2\n",
    "    Vec_key = Vec_key.view(batch,context_length, heads, dim_head)\n",
    "    Vec_value = Vec_value.view(batch,context_length, heads, dim_head)\n",
    "    \n",
    "    print('Vec_query Shape after unrolling: ', Vec_query.shape)\n",
    "\n",
    "    #Step 7: Transform or Shuffle the dimensions of the smaller projections to make the tensors situable for attention.\n",
    "    Vec_query = Vec_query.transpose(1,2) #Shape: b, 2, 6, 2\n",
    "    Vec_key = Vec_key.transpose(1,2)\n",
    "    Vec_value = Vec_value.transpose(1,2)\n",
    "\n",
    "    print('Transposed unrolled Vec_query: ', Vec_query.shape)\n",
    "\n",
    "    print('\\nTensors ready for applying attention.. Yayyyy!!!!\\n')\n",
    "\n",
    "    #Step 8: Compute un-normalized self-attention score [The matrix multiplication is carried out between the 2 last dimensions (context_length, dim_head) \n",
    "                                                        # and then repeated for the individual heads ]\n",
    "    attention_score = Vec_query @ Vec_key.transpose(2,3)   #Shape: Last 2 dim --> query: 6x2 and key.T: 2x6\n",
    "    print('Shape of attention scores matrix: ', attention_score.shape)\n",
    "\n",
    "    #Step 9: Create the mask for causal attention:\n",
    "    mask = torch.triu(torch.ones(context_length,context_length), diagonal=1)\n",
    "    \n",
    "    #Step 10: Truncate the original mask to context length size and convert to boolean\n",
    "    mask = mask.bool()[:context_length, :context_length]\n",
    "\n",
    "    #Step 11: Apply the mask to get the masked un-normalized attention scores\n",
    "    masked_scores = attention_score.masked_fill(mask, -torch.inf)\n",
    "\n",
    "    #Step 12: Compute normalized and scaled attention weights\n",
    "    dim_k = Vec_key.shape[-1]\n",
    "    attention_weight = torch.softmax(masked_scores / dim_k**0.5, dim=-1)\n",
    "\n",
    "    #Step 13: Create the dropout layer\n",
    "    torch.manual_seed(123)\n",
    "    dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    #Step 14: Add the dropouts on the attention weights to get sparse causal attention weights\n",
    "    sparse_attention_weights = dropout(attention_weight) #Shape: b,heads,6,6\n",
    "    print('Shape of sparse attention weights matrix: ', sparse_attention_weights.shape)\n",
    "\n",
    "    #Step 15: Compute the context vectors using the sparse attention weights and value vector\n",
    "    context_vector = sparse_attention_weights @ Vec_value #Shape: last 2 dim --> weights: 6x6 and value: 6x2\n",
    "    print('Shape of context vector: ', context_vector.shape) #Shape: b, heads, 6, 2 or b,2,6,2\n",
    "\n",
    "    #Step 16: Perform re-transpositon of the context vector to make the tensor situable for rolling the last 2 dimension into 1 dimension (dim_out)\n",
    "    context_vector = context_vector.transpose(1,2) #Shape: b,6,2,2\n",
    "\n",
    "    #Step 17: Rolling the last two dimension back into 1 to make the tensor situable for final output: dim_out = heads * dim_head\n",
    "    context_vector = context_vector.contiguous().view(batch,context_length,dim_out)\n",
    "\n",
    "    #Step 18: Create the final linear projection layer\n",
    "    final_projection = torch.nn.Linear(dim_out, dim_out)\n",
    "\n",
    "    #Step 19: Perform the final projection to get the FINAL Context Vector\n",
    "    context_vector = final_projection(context_vector)\n",
    "    print('Final CONTEXT VECTOR SHAPE: ', context_vector.shape)\n",
    "\n",
    "    #Step 20: Perform a final assertion check to make sure dim_out = heads * dim_head\n",
    "    assert context_vector.shape[-1] == heads * dim_head\n",
    "    print('\\nMULTI-HEAD CAUSAL SELF-ATTENTION WITH WEIGHT SPLIT DONE... YAYYYYYYYYYYY!!!!!!!!!!!!!!!!')    \n",
    "\n",
    "    return context_vector   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor:  torch.Size([2, 6, 3])\n",
      "Dimension of each heads:  2\n",
      "Context length:  6\n",
      "Input embedding dimension detected:  3\n",
      "Final context vector dimension (required):  4\n",
      "Vec_query Shape before unrolling:  torch.Size([2, 6, 4])\n",
      "Vec_query Shape after unrolling:  torch.Size([2, 6, 2, 2])\n",
      "Transposed unrolled Vec_query:  torch.Size([2, 2, 6, 2])\n",
      "\n",
      "Tensors ready for applying attention.. Yayyyy!!!!\n",
      "\n",
      "Shape of attention scores matrix:  torch.Size([2, 2, 6, 6])\n",
      "Shape of sparse attention weights matrix:  torch.Size([2, 2, 6, 6])\n",
      "Shape of context vector:  torch.Size([2, 2, 6, 2])\n",
      "Final CONTEXT VECTOR SHAPE:  torch.Size([2, 6, 4])\n",
      "\n",
      "MULTI-HEAD CAUSAL SELF-ATTENTION WITH WEIGHT SPLIT DONE... YAYYYYYYYYYYY!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4007,  0.4685, -0.1581, -0.0175],\n",
       "         [-0.3662,  0.4427, -0.1431, -0.0633],\n",
       "         [-0.3570,  0.4283, -0.1400, -0.0803],\n",
       "         [-0.3568,  0.4215, -0.1364, -0.0536],\n",
       "         [-0.3571,  0.3630, -0.1364, -0.0600],\n",
       "         [-0.3555,  0.3938, -0.1337, -0.0424]],\n",
       "\n",
       "        [[-0.4007,  0.4685, -0.1581, -0.0175],\n",
       "         [-0.3662,  0.4427, -0.1431, -0.0633],\n",
       "         [-0.3570,  0.4283, -0.1400, -0.0803],\n",
       "         [-0.3568,  0.4215, -0.1364, -0.0536],\n",
       "         [-0.3571,  0.3630, -0.1364, -0.0600],\n",
       "         [-0.3555,  0.3938, -0.1337, -0.0424]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.stack((inputs, inputs), dim=0)\n",
    "print('Shape of input tensor: ', input_tensor.shape)\n",
    "dim_out = 4\n",
    "heads = 2\n",
    "dropout_rate = 0.0\n",
    "mha_causal_attention(input_tensor, heads, dim_out,dropout_rate, qkv_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-cuda12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
